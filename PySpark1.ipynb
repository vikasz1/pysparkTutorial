{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":61029,"status":"ok","timestamp":1723658748577,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"kpfkWQG1DvBQ","outputId":"73d861f2-9a9a-41df-d5d5-6ca5e1eedf00"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n","  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n","Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812367 sha256=22a1d7820da4b35f6cfb83fabcc5c835bac522f9fcaad01eae52e67538bacdeb\n","  Stored in directory: /home/vikas-maury/.cache/pip/wheels/cf/c0/b9/f147f4220fd1d9277d0981b88b35b26f03ad910fffd60013a6\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.7 pyspark-3.5.2\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":405,"status":"ok","timestamp":1723658766361,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"KqNEU-TTEDEz"},"outputs":[],"source":["import  pyspark"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"collapsed":true,"executionInfo":{"elapsed":8,"status":"ok","timestamp":1723658985313,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"Ihv50qY-EKa8","outputId":"8122f814-0e21-4caf-9f1a-9ad26c730cd3"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pandas'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"]}],"source":["import pandas as pd\n","pd.read_csv('sample_data.csv')\n","#"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7306,"status":"ok","timestamp":1723659084529,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"ntMimI7bFNl2"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/08/15 00:27:27 WARN Utils: Your hostname, vikas-maury-1-0 resolves to a loopback address: 127.0.1.1; using 192.168.1.42 instead (on interface wlp2s0)\n","24/08/15 00:27:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/08/15 00:27:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["# Create a Pyspark Session\n","from pyspark.sql import SparkSession\n","spark=SparkSession.builder.appName('Practise').getOrCreate()"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"collapsed":true,"executionInfo":{"elapsed":411,"status":"ok","timestamp":1723659104479,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"GznDNFJ6Fcms","outputId":"a9409ba3-a0b8-429b-9a6e-a7f534a82bf6"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://192.168.1.42:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.5.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Practise</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x79e4374cbdd0>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7332,"status":"ok","timestamp":1723659157899,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"DHSxBXGhFqyV"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/08/15 00:27:46 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"]}],"source":["df_pyspark=spark.read.csv('sample_data.csv')"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1119,"status":"ok","timestamp":1723659389902,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"o2pftdGEF2IG","outputId":"622dc20b-78b8-4e12-8bd6-216a7bac4c2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+---+----------+\n","|   Name|Age|Experience|\n","+-------+---+----------+\n","|  Vikas| 24|         0|\n","|Adityan| 32|         8|\n","|Bhavani| 43|        15|\n","+-------+---+----------+\n","\n"]}],"source":["df_pyspark = spark.read.option('header','true').csv(\"sample_data.csv\")\n","df_pyspark.show()"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1723659395002,"user":{"displayName":"Vikas Maury","userId":"05366182623802963751"},"user_tz":-330},"id":"VmDgHZt8GoAz","outputId":"661a7629-9016-4469-8a37-dec987f7444f"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Age: string (nullable = true)\n"," |-- Experience: string (nullable = true)\n","\n"]}],"source":["df_pyspark.printSchema()"]},{"cell_type":"markdown","metadata":{"id":"1-MzqyB5HBLQ"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP+Vu58xJcisGJXgQnnsF6I","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":0}
